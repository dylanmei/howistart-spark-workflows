{
  "paragraphs": [
    {
      "text": "%spark \n\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\n\n// example log line\n// 2016-08-14 10:02:29.378089 IP 10.0.1.12.44382 \u003e 52.91.17.255.443: tcp 60\n\nval lines \u003d spark.sparkContext.textFile(\"/var/lib/data/tcpdump.txt\")\nval fields \u003d  Array(\n    StructField(\"datetime\", StringType, false),\n    StructField(\"ip\", StringType, false),\n    StructField(\"len\", IntegerType, false))\n\nval metrics \u003d lines\n  .map(_.split(\" \"))\n  .filter(_.size \u003d\u003d 8)\n  .map(attrs \u003d\u003e Row(attrs(0) + \"T\" + attrs(1), attrs(5), attrs(7).toInt))\n\nval df \u003d spark.createDataFrame(metrics, StructType(fields))\ndf.createOrReplaceTempView(\"tcpdump\")",
      "dateUpdated": "Aug 17, 2016 1:38:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1471195257765_-555373325",
      "id": "20160814-172057_996956259",
      "dateCreated": "Aug 14, 2016 5:20:57 AM",
      "dateStarted": "Aug 17, 2016 1:38:46 PM",
      "dateFinished": "Aug 17, 2016 1:38:49 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql select * from tcpdump",
      "dateUpdated": "Aug 16, 2016 12:54:16 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": true,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "forceY": true
        },
        "enabled": true,
        "editorMode": "ace/mode/sql",
        "helium": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1471203360425_1591095266",
      "id": "20160814-193600_649498835",
      "dateCreated": "Aug 14, 2016 7:36:00 AM",
      "dateStarted": "Aug 16, 2016 12:54:16 PM",
      "dateFinished": "Aug 16, 2016 12:54:16 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\n\n// example log line\n// 2016-08-14 10:02:29.378089 IP 10.0.1.12.44382 \u003e 52.91.17.255.443: tcp 60\n\ncase class Metric(timestamp: String, ip: String, port: Int, len: Int)\n\nval lines \u003d spark.sparkContext.textFile(\"/var/lib/data/tcpdump.txt\")\nval metrics \u003d lines\n  .map(_.split(\" \"))\n  .filter(_.size \u003d\u003d 8)\n  .map(attrs \u003d\u003e {\n        val timestamp \u003d attrs(0) + \"T\" + attrs(1)\n        val remote \u003d attrs(5)\n        val (ip, port) \u003d remote.splitAt(remote.lastIndexOf(\".\"))\n\n        Metric(timestamp,\n            ip.replaceAll(\"\\\\.\", \"-\"),\n            port.stripPrefix(\".\").stripSuffix(\":\").toInt,\n            attrs(7).toInt)\n     })\n\nval df \u003d spark.createDataFrame(metrics)\ndf.createOrReplaceTempView(\"tcpdump\")",
      "dateUpdated": "Aug 17, 2016 1:38:45 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1471204661573_-709464547",
      "id": "20160814-195741_1573365906",
      "dateCreated": "Aug 14, 2016 7:57:41 AM",
      "dateStarted": "Aug 16, 2016 12:56:42 PM",
      "dateFinished": "Aug 16, 2016 12:56:46 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql select * from tcpdump",
      "dateUpdated": "Aug 16, 2016 12:58:20 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "lineChart",
          "height": 300.0,
          "optionOpen": true,
          "keys": [
            {
              "name": "timestamp",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "len",
              "index": 3.0,
              "aggr": "sum"
            }
          ],
          "groups": [
            {
              "name": "ip",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "scatter": {
            "xAxis": {
              "name": "timestamp",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/sql",
        "helium": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1471273569763_439555946",
      "id": "20160815-150609_1610041277",
      "dateCreated": "Aug 15, 2016 3:06:09 AM",
      "dateStarted": "Aug 16, 2016 12:57:04 PM",
      "dateFinished": "Aug 16, 2016 12:57:04 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport com.snowplowanalytics.maxmind.iplookups.IpLookups\n\ncase class Metric(timestamp: String, ip: String, port: Int, city: String, country: String, location: String, len: Int)\n\nval lines \u003d spark.sparkContext.textFile(\"/var/lib/data/tcpdump.txt\")\nval metrics \u003d lines\n  .map(_.split(\" \"))\n  .filter(_.size \u003d\u003d 8)\n  .map(attrs \u003d\u003e {\n        val timestamp \u003d attrs(0) + \"T\" + attrs(1)\n        val remote \u003d attrs(5)\n        val (ip, port) \u003d remote.splitAt(remote.lastIndexOf(\".\"))\n\n        val ipl \u003d IpLookups(geoFile \u003d Some(\"/var/lib/geo/GeoLiteCity.dat\"))\n        \n        val (city, country, loc) \u003d ipl.performLookups(ip)._1 match {\n            case Some(geo) \u003d\u003e (geo.city, Option(geo.countryName), Option(geo.latitude + \",\" + geo.longitude))\n            case _ \u003d\u003e (None, None, None)\n        }\n        \n        Metric(timestamp,\n            ip.replaceAll(\"\\\\.\", \"-\"),\n            port.stripPrefix(\".\").stripSuffix(\":\").toInt,\n            city.getOrElse(\"\"),\n            country.getOrElse(\"\"),\n            loc.getOrElse(\"0,0\"),\n            attrs(7).toInt)\n     })\n\nval df \u003d spark.createDataFrame(metrics)\ndf.createOrReplaceTempView(\"tcpdump\")",
      "dateUpdated": "Aug 17, 2016 1:38:45 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1471274267728_2129773712",
      "id": "20160815-151747_815741566",
      "dateCreated": "Aug 15, 2016 3:17:47 AM",
      "dateStarted": "Aug 16, 2016 5:37:48 PM",
      "dateFinished": "Aug 16, 2016 5:37:53 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql select * from tcpdump",
      "dateUpdated": "Aug 16, 2016 5:41:08 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sql"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1471354195161_-14643599",
      "id": "20160816-132955_1708791825",
      "dateCreated": "Aug 16, 2016 1:29:55 PM",
      "dateStarted": "Aug 16, 2016 5:41:08 PM",
      "dateFinished": "Aug 16, 2016 5:41:24 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%angular\n\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.5/leaflet.css\" /\u003e\n\u003cdiv id\u003d\"map\" style\u003d\"height: 400px; width: 100%\"\u003e\u003c/div\u003e\n\n\u003cscript type\u003d\"text/javascript\"\u003e\nfunction initMap() {\n    var map \u003d L.map(\u0027map\u0027).setView([30.00, -30.00], 2)\n\n    L.tileLayer(\u0027http://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\u0027, {\n        attribution: \u0027Map data \u0026copy; \u003ca href\u003d\"http://openstreetmap.org\"\u003eOpenStreetMap\u003c/a\u003e contributors\u0027,\n        maxZoom: 12, minZoom: 2\n    }).addTo(map)\n\n    var markers \u003d L.layerGroup().addTo(map);\n    var el \u003d angular.element($(\u0027#map\u0027).parent(\u0027.ng-scope\u0027));\n    angular.element(el).ready(function() {\n        window.locationWatcher \u003d el.scope().compiledScope.$watch(\u0027tcpdump\u0027, function(newValue, oldValue) {\n            markers.clearLayers() //-- if you want to only show new data clear the layer first\n            angular.forEach(newValue, function(latlon) {\n                if (latlon.length \u003e 0) {\n                    console.log(latlon)\n                    L.marker(latlon.split(\",\")).addTo(markers)\n                }\n            })\n        })\n    })\n}\n\nif (window.locationWatcher) {\n    // clear existing watcher otherwise we\u0027ll have duplicates\n    window.locationWatcher();\n}\n\n// ensure we only load the script once, seems to cause issues otherwise\nif (window.L) {\n    initMap();\n} else {\n    console.log(\u0027Loading Leaflet library\u0027);\n    var sc \u003d document.createElement(\u0027script\u0027);\n    sc.type \u003d \u0027text/javascript\u0027;\n    sc.src \u003d \u0027https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.5/leaflet.js\u0027;\n    sc.onload \u003d initMap;\n    sc.onerror \u003d function(err) { alert(err); }\n    document.getElementsByTagName(\u0027head\u0027)[0].appendChild(sc);\n}\n\u003c/script\u003e",
      "dateUpdated": "Aug 18, 2016 2:33:49 AM",
      "config": {
        "colWidth": 8.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1471361328814_970081481",
      "id": "20160816-152848_63847801",
      "result": {
        "code": "SUCCESS",
        "type": "ANGULAR",
        "msg": "\u003clink rel\u003d\"stylesheet\" href\u003d\"https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.5/leaflet.css\" /\u003e\n\u003cdiv id\u003d\"map\" style\u003d\"height: 400px; width: 100%\"\u003e\u003c/div\u003e\n\n\u003cscript type\u003d\"text/javascript\"\u003e\nfunction initMap() {\n    var map \u003d L.map(\u0027map\u0027).setView([30.00, -30.00], 2)\n\n    L.tileLayer(\u0027http://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\u0027, {\n        attribution: \u0027Map data \u0026copy; \u003ca href\u003d\"http://openstreetmap.org\"\u003eOpenStreetMap\u003c/a\u003e contributors\u0027,\n        maxZoom: 12, minZoom: 2\n    }).addTo(map)\n\n    var markers \u003d L.layerGroup().addTo(map);\n    var el \u003d angular.element($(\u0027#map\u0027).parent(\u0027.ng-scope\u0027));\n    angular.element(el).ready(function() {\n        window.locationWatcher \u003d el.scope().compiledScope.$watch(\u0027tcpdump\u0027, function(newValue, oldValue) {\n            markers.clearLayers() //-- if you want to only show new data clear the layer first\n            angular.forEach(newValue, function(latlon) {\n                if (latlon.length \u003e 0) {\n                    console.log(latlon)\n                    L.marker(latlon.split(\",\")).addTo(markers)\n                }\n            })\n        })\n    })\n}\n\nif (window.locationWatcher) {\n    // clear existing watcher otherwise we\u0027ll have duplicates\n    window.locationWatcher();\n}\n\n// ensure we only load the script once, seems to cause issues otherwise\nif (window.L) {\n    initMap();\n} else {\n    console.log(\u0027Loading Leaflet library\u0027);\n    var sc \u003d document.createElement(\u0027script\u0027);\n    sc.type \u003d \u0027text/javascript\u0027;\n    sc.src \u003d \u0027https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.5/leaflet.js\u0027;\n    sc.onload \u003d initMap;\n    sc.onerror \u003d function(err) { alert(err); }\n    document.getElementsByTagName(\u0027head\u0027)[0].appendChild(sc);\n}\n\u003c/script\u003e"
      },
      "dateCreated": "Aug 16, 2016 3:28:48 PM",
      "dateStarted": "Aug 18, 2016 2:33:49 AM",
      "dateFinished": "Aug 18, 2016 2:33:49 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n\nval locations \u003d rows\n    .groupBy(_.location)\n    .map(_._1)\n\nz.angularBind(\"tcpdump\", locations.collect())",
      "dateUpdated": "Aug 18, 2016 2:34:09 AM",
      "config": {
        "colWidth": 4.0,
        "graph": {
          "mode": "table",
          "height": 226.5,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1471355443352_171348295",
      "id": "20160816-135043_427031445",
      "dateCreated": "Aug 16, 2016 1:50:43 PM",
      "dateStarted": "Aug 16, 2016 5:37:59 PM",
      "dateFinished": "Aug 16, 2016 5:38:37 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql ",
      "dateUpdated": "Aug 16, 2016 3:29:04 PM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1471361344513_-875158920",
      "id": "20160816-152904_1927819405",
      "dateCreated": "Aug 16, 2016 3:29:04 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "1. TcpDump with Spark SQL",
  "id": "2BSX6Z8SM",
  "lastReplName": {
    "value": "angular"
  },
  "angularObjects": {
    "2BV243892:shared_process": [],
    "2BUFW961K:shared_process": [],
    "2BW63SUXE:shared_process": [],
    "2BVHZMGGD:shared_process": [],
    "2BVGC7XJ2:shared_process": [],
    "2BVEDXHCB:shared_process": [],
    "2BV4U2ARY:shared_process": [],
    "2BVYPTTNB:shared_process": [],
    "2BVW9F1TD:shared_process": []
  },
  "config": {},
  "info": {}
}
